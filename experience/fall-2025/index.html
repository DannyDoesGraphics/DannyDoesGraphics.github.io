<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><meta content="I <3 data" name=description><title>Data analysis @ ComPeters</title><link href=/img/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/img/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/img/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><style>body{--primary-color:#88c0d0;--primary-pale-color:#88c0d020;--text-color:#d8dee9;--text-pale-color:#eceff4;--bg-color:#2e3440;--highlight-mark-color:#5e81ac35;--callout-note-color:#88c0d0;--callout-important-color:#b48ead;--callout-warning-color:#d08770;--callout-alert-color:#bf616a;--callout-question-color:#81a1c1;--callout-tip-color:#a3be8c;--main-font:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:768px;--main-max-width:768px;--avatar-size:60px;--icon-size:20px;--homepage-font-size:16px;--homepage-line-height:1.75;--paragraph-font-size:16px;--paragraph-line-height:1.75;--aside-font-size:15px;--img-border-radius:0px;--callout-border-radius:0px;--detail-border-radius:0px;--dark-mode-img-brightness:.75;--dark-mode-chart-brightness:.75;--inline-code-border-radius:2px;--inline-code-bg-color:var(--primary-pale-color);--block-code-border-radius:0px;--block-code-border-color:var(--primary-color);--detail-border-color:var(--primary-color)}</style><link href=/main.css rel=stylesheet><link href=/hl-light.css id=hl rel=stylesheet><body class=prose-page><script>const theme = sessionStorage.getItem('theme');
    const match = window.matchMedia("(prefers-color-scheme: dark)").matches
    if ((theme && theme == 'dark') || (!theme && match)) {
      document.body.classList.add('dark');
      const hl = document.querySelector('link#hl');
      if (hl) hl.href = '/hl-dark.css';
    }</script><header class=blur><div id=header-wrapper><nav><a class=instant href=/>DannyDoesGraphics</a><button aria-label="toggle expand" class=separator id=toggler>::</button><span class="wrap left fold">{</span><a class="instant fold" href=/blog>blog</a><span class="wrap-separator fold">,</span><a class="instant fold" href=/projects>projects</a><span class="wrap-separator fold">,</span><a class=instant href=/experience>experience</a><span class="wrap-separator fold">,</span><a class="instant fold" href=/about>about</a><span class="wrap right fold">} ;</span></nav><div id=btns></div></div></header><div id=wrapper><main><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' id=copy-cfg style=display:none></div><article data-backlink-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M9.41421 8L18.0208 16.6066L16.6066 18.0208L8 9.41421V17H6V6H17V8H9.41421Z" fill="currentColor"></path></svg>' class=prose><h1 id=introduction>Introduction<a aria-label="Anchor link for: introduction" class=zola-anchor href=#introduction style=visibility:hidden>#</a></h1><p>During the fall 2025 semester, rather than doing an academic term, I did a co-op sequence as part of my co-op degree. The company I worked at was <em>ComPeters</em>, a SaaS company that provided software for utility companies to send work orders to contractors who demark utility lines.<h2 id=my-work>My work<a aria-label="Anchor link for: my-work" class=zola-anchor href=#my-work style=visibility:hidden>#</a></h2><p>Due to some <em>unforeseen circumstances</em>, I was unable to work on the core code in production at ComPeters. However, alongside another co-op student, I was given a project to lead where we analyzed NFL game data to predict pre-kickoff win probabilities.<p>This was a very open-ended project: so long as we could convince stakeholders that our design decisions were logical, they would permit it. However, this would both be a blessing and a curse later down the line...<h2 id=chapter-1-the-turtle-and-rabbit>Chapter 1: The turtle and rabbit<a aria-label="Anchor link for: chapter-1-the-turtle-and-rabbit" class=zola-anchor href=#chapter-1-the-turtle-and-rabbit style=visibility:hidden>#</a></h2><p>We were told that our backend and data pipeline had to exist solely on AWS. I had prior experience working with infrastructure through locally hosting Kubernetes for a homelab, but not cloud. However, my project teammate did not. This presented a big problem I failed to catch on early: I went too fast.<p>During project planning, I skipped over planning the backend and data pipeline architecture, jumping directly to creating the prediction models for games whilst my teammate was confused and left behind. Fortunately, my boss did notice, and we resolved it by using tasks in the form of AGILE development strategies. Informally, we treated it more as a task-based list we just had to do, and did weekly stand-ups together to see where we were on progress. This allowed me to help him on certain problems he was struggling with such as IaC (Infrastructure as Code).<h3 id=my-job>My job<a aria-label="Anchor link for: my-job" class=zola-anchor href=#my-job style=visibility:hidden>#</a></h3><p>After finally getting some basic team organization running, my project member and I decided that I would focus more on setting up the initial project through Pulumi to handle IaC and creating a basic PostgreSQL migration script, as well as handling the data pipeline + model building given I had taken some statistics courses. Meanwhile, he would handle creating the backend "interface" to create logging and insights into weekly predictions made by the model.<h2 id=chapter-2-data-analytics>Chapter 2: Data analytics<a aria-label="Anchor link for: chapter-2-data-analytics" class=zola-anchor href=#chapter-2-data-analytics style=visibility:hidden>#</a></h2><p>This is where the project became <em>fun</em>. Sadly, I can only publish a small subset of the data analyzed due to employer restrictions. Originally, the idea was to use a regression and/or a genetic algorithm to initially develop the model to do predictions. However, upon analysis of EPA data using Pandas, we discovered our data set was highly collinear and scattered, making it difficult for regression models to work with. For example, injury data was sparse and unreliable, so we couldn't rely on it.<figure><img alt="Correlation matrix showing collinearity between EPA metrics" src=/images/experience/fall-2025/colinearity.png><figcaption>Correlation Matrix showing different predictors and colinearity that arose</figcaption></figure><blockquote><p>Why not genetic algorithms?<p>They can be used, and we do use a variation of them later. However, the biggest problem of genetic algorithms is the sheer computational time it takes. It requires permutating over millions of possible combinations of predictors to end up with a reasonable model. In contrast, machine learning methods such as gradient boost more "quickly" find optimal decision trees at less of a performance cost. Furthermore, gradient boosted trees perform overwhelmingly well on sparse tabular data which was the data we were handling here. (Grinsztajn et al., 2022)</blockquote><p>I had done a quick "proto-presentation" to the stakeholders involved showing my findings. As a result, we were able to quickly pivot away from regression models, which struggle on sparse and collinear data, and genetic algorithms, which have a high computation cost, and are less optimal compared to boosting techniques. This experience really showed me the value of documenting my results, and quantifying them in a way that is easily understandable to stakeholders.<h3 id=basic-correlation>Basic correlation<a aria-label="Anchor link for: basic-correlation" class=zola-anchor href=#basic-correlation style=visibility:hidden>#</a></h3><p>However, just because we have a boosting model, does not imply that we can just blindly throw data at it. In fact, throwing too much data may harm model performance. Thus, feature selection and creation is imperative to create competent models.<h3 id=betting-markets>Betting markets<a aria-label="Anchor link for: betting-markets" class=zola-anchor href=#betting-markets style=visibility:hidden>#</a></h3><p>The biggest flaw with betting markets is that they're <em>too</em> good of a signal. Many times, betting markets may dominate and become the only signal used by the model.<div style=flex-wrap:wrap;justify-content:center;gap:1rem;display:flex><figure style=text-align:center;flex:1;min-width:300px><img alt="Betting market Brier score over weeks" src=/images/experience/fall-2025/brier_moneyline_chart.png style=max-width:100%><figcaption>Betting market performance over NFL weeks</figcaption></figure><figure style=text-align:center;flex:1;min-width:300px><img alt="Betting market Brier score" src=/images/experience/fall-2025/brier_moneyline_score.png style=max-width:100%><figcaption>Betting market Brier score results</figcaption></figure></div><h3 id=elo>ELO?<a aria-label="Anchor link for: elo" class=zola-anchor href=#elo style=visibility:hidden>#</a></h3><p>ELO is a zero-sum scoring system. When two teams compete, we give the winner <code>n</code> points and subtract <code>n</code> points from the loser. As a result, we can build a sufficient scoring system. An initial analysis shows us:<div style=flex-wrap:wrap;justify-content:center;gap:1rem;display:flex><figure style=text-align:center;flex:1;min-width:300px><img alt="Negative Glicko + ELO calibration" src=/images/experience/fall-2025/-glicko_elo_calibration.png style=max-width:100%><figcaption>-Glicko + ELO model calibration</figcaption></figure><figure style=text-align:center;flex:1;min-width:300px><img alt="Negative Glicko + ELO score" src=/images/experience/fall-2025/-glicko_elo_score.png style=max-width:100%><figcaption>-Glicko + ELO model performance scores</figcaption></figure></div><h3 id=elo-mov>ELO MoV?<a aria-label="Anchor link for: elo-mov" class=zola-anchor href=#elo-mov style=visibility:hidden>#</a></h3><p>We can further improve upon ELO by potentially adding margin of victory. The more a team wins by, the more we reward them. This <em>surely</em> improves it, right? Right?<div style=flex-wrap:wrap;justify-content:center;gap:1rem;display:flex><figure style=text-align:center;flex:1;min-width:300px><img alt="ELO calibration plot" src=/images/experience/fall-2025/elo_mov_calibration.png style=max-width:100%><figcaption>ELO model calibration</figcaption></figure><figure style=text-align:center;flex:1;min-width:300px><img alt="ELO score" src=/images/experience/fall-2025/elo_mov_score.png style=max-width:100%><figcaption>ELO model performance scores</figcaption></figure></div><p>Not really, it causes a fair bit of overfitting too for some odd reason. I believe this has to do more with parameter tuning, rather than margin of victory being <em>that</em> decisive. Regardless, we do find overall margin of victory may be a false flag indicator.<h3 id=glicko-failure>Glicko: Failure<a aria-label="Anchor link for: glicko-failure" class=zola-anchor href=#glicko-failure style=visibility:hidden>#</a></h3><p>Glicko-2 was an improvement upon ELO. The idea was to also quantify time since last match (this could capture roster changes between seasons!), volatility, and uncertainty.<div style=flex-wrap:wrap;justify-content:center;gap:1rem;display:flex><figure style=text-align:center;flex:1;min-width:300px><img alt="Glicko + ELO calibration" src=/images/experience/fall-2025/glicko_elo_calibration.png style=max-width:100%><figcaption>Glicko + ELO model calibration</figcaption></figure><figure style=text-align:center;flex:1;min-width:300px><img alt="Glicko + ELO score" src=/images/experience/fall-2025/glicko_elo_score.png style=max-width:100%><figcaption>Glicko + ELO model performance scores</figcaption></figure></div><p>But glicko-2 actually does worse! Why? Glicko-2 is optimized for individual players, not teams. On a team level, Glicko-2 struggles because volatility and uncertainty wildly swing with team composition changes that can't be captured by single scalars.<p>In hindsight, I should have better researched Glicko-2 before blindly applying it, recognizing the limitations of the additional scalars it adds. As a final step to the model, I decided to recklessly apply hyperparameter optimization to the boosting model we're using. In truth, we should have relied on fine tuning given our data set of ~4000 games is easy to overfit.<h2 id=chapter-3-implementation>Chapter 3: Implementation<a aria-label="Anchor link for: chapter-3-implementation" class=zola-anchor href=#chapter-3-implementation style=visibility:hidden>#</a></h2><p>After finalizing the model, which gave us a Brier score of <code>0.217</code>, we needed to actually put the model into our backend. The idea was to create an EventBridge which would run yearly training runs on the previous season's data with another EventBridge job linked to a Python lambda which will do data processing.<figure><img alt="AWS architecture" src=/images/experience/fall-2025/nfl_wp_architecture.png><figcaption>AWS backend architecture</figcaption></figure><p>The data processing lambda would ensure we would cache weekly NFL game results so that both running and training the model could grab pre-computed data rather than recalculate it, saving AWS credits.<p>The entire implementation was done in Python. Surprisingly, this initial architecture plan is more or less the architecture still used. The most notable change was to the schema where we had to create a log in the database of each prediction and the model responsible for it for observability.<h2 id=chapter-4-reflection>Chapter 4: Reflection<a aria-label="Anchor link for: chapter-4-reflection" class=zola-anchor href=#chapter-4-reflection style=visibility:hidden>#</a></h2><p>A Brier score of <code>0.217</code> isn't horrible; in fact, we're around 67% correct on our test data set. The problem is that we're using betting markets too which perform at a Brier score of <code>0.210</code> and an accuracy of 66%. If I could rewind time, I would look harder into regression analysis. Regression isn't the worst so long as our data doesn't have gaps or is highly colinear. In fact, many <a rel="nofollow noreferrer" href=https://www.nfeloapp.com/>other models</a> actually perform better (nflelo performed around ~0.2095 which is <em>slightly</em> better than betting markets), by relying more on regression analysis.<p>Furthermore, the fact that our model did <em>worse</em> even with features that would in theory improve the Brier score strongly indicates to me that the boosting model we're using is overfitting to some degree, weighing certain features higher than they should be. If I had more time, I would've spent it better fine tuning the parameters, however, with only 4000 games, even human tuning risks overfitting.<p>On the personal side, good did come out of it. Learning how to use Python beyond just small simple projects was really fun, and especially so with Pandas to process large sets of data. It was great coordinating and leading a small team to develop the win predictor, and resolving misunderstandings. Furthermore, the presentations I had done went extremely well! We were able to convey a lot of core documentation, but also project plans, and reasoning in both presentations.<h2 id=chapter-5-end>Chapter 5: End<a aria-label="Anchor link for: chapter-5-end" class=zola-anchor href=#chapter-5-end style=visibility:hidden>#</a></h2><p>My time at ComPeters, primarily doing data analysis, was made amazing thanks to the new learning I had to do such as learning Pandas and AWS. I learnt a lot, especially beginner machine learning concepts, and this experience strongly prepared me for future courses like <em>Cloud Computing</em>. I was also able to build strong communication skills, both with stakeholders and project peers. I would like to thank my co-op supervisor, Shane Hart, for all the guidance and patience he provided.</article></div><footer><div class=copyright><p>Â© 2026 Danny Le</div><div class=credits>powered by <a rel="noreferrer noopener" href=https://www.getzola.org target=_blank>zola</a> and <a rel="noreferrer noopener" href=https://github.com/isunjn/serene target=_blank>serene</a></div></footer></main></div><script src=/js/lightense.min.js></script><script src=/js/main.js></script>